---
title: "Bayesian Logistic Regression Application in Diabetes Probability Prediction"
subtitle: "CapStone Project_2025"
author: "Namita Mishra (Advisor: Dr. Ashraf Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Literature Review

## Introduction

1.  Bayesian Hierarchical Model (Disease reclassification and
    prediction)**

What is the goal of the paper?

To develop a Bayesian hierarchical model for
multivariate longitudinal data to predict health status, trajectories,
and intervention effects at the individual level in PCORI mission to
address questions about health status from patients and clinicians.

Why is it important?

Healthcare data (DNA sequences, functional images of the brain,
patient-reported outcomes, and electronic health records with patients’
sequences of health measurements, diagnoses, and treatments) are
complex, and the standard approaches are not adequate for clinical data
analysis. 

How is it solved?

Use of Electronic health records (EHRs) to improve the diagnostic
accuracy and predict treatment effects. 
Visualizations of posterior distributions are immediately understood by clinicians and patients as relevant to their decision. 
Combining prior knowledge and patient data with evidence could predict the
patient’s health status, trajectory, and/or likely benefits of
intervention. 

Use of Bayesian hierarchical regression on multivariate
longitudinal patient data (R-packages) developed two-levels (1)time
within person and (2) persons within a population along with co-variates
and interventions by combining exogenous (eg, age, clinical history)
factors and endogenous (eg, current treatment) variables on the
individual’s multivariate health measurements and the effects of health
measurements at one time on subsequent interventions.It provided posterior distribution for predictor variables and an estimate of the marginal distribution of the regression coefficients for each coefficient. 
A large sample (based on likelihood) dominates the prior distribution for regression coefficients and Bayesian hierarchical model is a likelihood-based approach and uses priors providing sensitivities. The integration of Markov chain Monte Carlo (MCMC) estimates the posterior distributions, that avoided missing data and complex outcome measurements.

Results

- Applied in pneumonia etiology in children, prostate cancer, and
mental disorders to identify low-risk patient population, reduced the risk of over-treatment, complications,adverse effects, and financial burden for patients (Disease
Reclassification). Prostate cancer software was then implemented within
the JHM HER.

Limitation:

Models were entirely parametric, and recommended for an extensions to nonparametric or more
flexible parametric models for neuroimage or genomic data. Recommendations are to address unmet need across a larger, diverse population, use in autoimmune diseases, sudden cardiac arrest, and diabetes and to embed tools to acquire and use the most relevant information for a better outcome at an affordable cost.
@Zeger2020

2.  Bayesian Inference (parametric vs non-parametric)

What is the goal of the paper?

The study calculated the posterior probability of disease diagnosis by
applying Bayesian inference to develop three modules comparing parametric
(with a fixed set of parameters) and nonparametric distributions (which
do not make a priori assumptions). The National Health and Nutrition
Examination Survey data from two separate diagnostic tests on both
diseased and non-diseased populations was used for model development.

Why is it important?

Medical diagnosis, treatment, and management decisions are crucial, and
conventional methods based on clinical criteria and fixed
numerical thresholds limit the information captured on intricate relationship between diagnostic tests and the varying prevalence of diseases. 
The probability distributions associated with quantitative diagnostic test outcomes have some overlap between the diseased and nondiseased groups. The dichotomous method fails to capture the complexity and heterogeneity of disease presentations across diverse populations.  The applicability of the normal distribution (conventional method) is critiqued, especially in dealing with clinical measurands having skewness, bimodality, or multimodality.

How is it solved?

Bayesian nonparametric (vs parametric) diagnostic modeling is a Flexible distributional modeling for test outcomes; posterior disease probs

The study models developed employed Bayesian inference for posterior probability calculation of disease diagnosis in the Wolfram Language and integrated prior probabilities of disease with distributions of diagnostic measurands in both diseased and nondiseased populations. The approach enabled the evaluation of combined data from multiple diagnostic tests that improved the diagnostic
accuracy, precision, and adaptability. The model showed flexibility,
adaptability, and versatility in the diagnostic.

Results

Nonparametric Bayesian models were reported  a better fit for data distributions given the limited existing literature, and was more robust in capturing complex data patterns. These models produced multimodal probability patterns for disease, unlike the bimodal, double-sigmoidal curves seen with parametric models.

Limitations

- Reliance on parametric models
- Limited scholarly publications and over-dependence on prior
  probabilities increasing the uncertainties, resulting in broader
  confidence intervals for posterior probabilities. 
- Systemic bias (unrepresentative datasets) compromises the accuracy of Bayesian
  calculations. 
- For Incomplete datasets, Bayesian methods combined with other statistical and computational techniques could enhance diagnostic capabilities.

- The foundational data is crucial to compare new diagnostic
  measurements. 
  Absence of normative data compromises the reliability
  and validity of Bayesian diagnostic methods. @Chatzimichail2023

3.  Bayesian methodology overview (stages, development and advantages)

What is the goal of the paper?

The stages of Bayesian analysis are depicted here specifying the
importance of the priors, data modeling, inferences, model checking and
refinement, selecting a proper sampling technique from a posterior
distribution, variational inferences, variable selection and its
application across various research fields. The study proposes
strategies for reproducibility and reporting standards, outlining an
updated WAMBS (when to Worry and how to Avoid the Misuse of Bayesian
Statistics) checklist emphasising the impact of Bayesian analysis on
artificial intelligence in the future.

Why is it important?

Bayesian statistics can be used across different fields (social sciences,
ecology, genetics, medicine)

How is it solved?

The study  describes categories of priors (informative, weakly
informative and diffuse) based on the degree of (un)certainty
(hyperparameters) surrounding the population parameter. 
The prior distribution with N( μ0 , σ\^ 20) where a larger variance represents a
greater amount of uncertainty surrounding.

Prior elicitation (experts, generic expert, data-based, sample data using maximum
likelihood or sample statistics, etc) - help construct a prior distribution 

A prior sensitivity analysis of the likelihood - examines different
forms of the model and assesses how the priors and the likelihood
align and impact on posterior estimates, reflecting variations not
captured by the prior or the likelihood alone.

Prior estimation -allows data-informed shrinkage, regularization or
influence algorithms towards a likely high-density region, and improves
estimation efficiency.

In a small sample i.e. less information, incorporation of priors strengthens the observed data and lends possible value(s) for the unknown parameter(s). Knowing the exact probabilistic
specification of the priors for a complex model with smaller sample sizes is important.
Frequentists do not consider the probability of the unknown parameters
as useful, and consider to be fixed; likelihood is considered as the the
conditional probability distribution p(y\|θ) of the data (y), given
fixed parameters (θ).

In Bayesian inference, unknown parameters (random
variables) have varied values, while the (observed) data have fixed
values. The likelihood is a function of θ for the fixed data y.
Therefore, the likelihood function summarizes a statistical model that
stochastically generates a range of possible values for θ and the
observed data y. With priors and the likelihood of the observed data,
the resulting posterior distribution provides an estimate of the unknown
parameters, capture primary factors and improve our
understanding.

Monte Carlo technique provides integrals of sampled values from a given
distribution through computer simulations. The packages BRMS and Blavaan
in R are used for the probabilistic programming language Stan.

Variables selection is best after checking correlations among the variables
in the model (Eg: gene-to-gene interaction to predict genes in
biomedical research).

Spatial and temporal variability are factored in Bayesian general linear
models. A posterior distribution can simulate new data conditional on
this distribution and assess providing valid predictions to be used for
extrapolating to future events.

Results and application

The Bayesian approach analyzes large-scale cancer genomic data,
identifies novel molecular changes in cancer initiation and progression,
the interactions between mutated genes and captured mutational
signatures, highlighting key genetic interactions components, allowing
genomic-based patient stratification both in clinical trials, in the
personalized use of therapeutics, and in understanding cancer and its
evolutionary processes.

Limitations

In temporal models, the spatial and/or temporal dependencies
(autocorrelation of parameters over time)is a challenge in posterior
inference. @VandeSchoot2021

4.  Bayesian Normal linear regression, Core parametric (conjugate)
    model with Normal–Inverse-Gamma prior

What is the goal of the paper?

The author provides guidance on Bayesian inference by performing
Bayesian Normal linear regression in metrology to calibrate instruments
to evaluate inter-laboratory comparisons in determining fundamental
constants. The study emphasizes on prior elicitation, analytical posteriors,
robustness checks.

Why is it important?

The measurement errors are assumed as additive, independent, and
identically distributed according to a Gaussian distribution (mean
zero and variance σ2), which is usually unknown and regression models cannot be uniquely formulated as a measurement function. Guide to the Expression of Uncertainty in Measurement (GUM) and its supplements are not applicable directly.

How is it solved?

Prior-
Bayesian inference account for addition of a priori information, which robustifies the analyses. The study emphasizes steps (prior elicitation, posterior calculation, and robustness to prior uncertainty and model adequacy) for the model development and about assumptions critical to Bayesian inference.

In Bayesian inference, all unknowns—observables (data) as well as
unobservables (parameters and auxiliary variables) are considered to be
random, are assigned probability distributions that summarizes the
available information, and update prior knowledge about the
unobservables with information about them contained in the data. The graphical representation of prior distribution and likelihood function, sensitivity analyses, or model checking enhances the elicitation and interpretation process.

For Normal linear regression problems 
(1) a family of prior distributions Normal inverse Gamma (NIG) distribution to a      posterior is from the same family of (NIG) distribution. 
    The NIG prior with known variance σ2 of observations is a conjugate prior         distribution. 
    Vague or non-informative prior distributions can be derived from NIG prior.

(2) alternative families of prior distributions (hierarchical priors)
    assign an additional layer of distributions to uncertain prior
    parameters or non-para- metric priors.

Bayesian inference is influenced by
-   the uncertainty in the transformation of prior knowledge to prior
    distributions
-   the assumptions of the statistical model
-   the mistakes in data acquisition

Results and Application

The knowledge from related previous experiments (Normal inverse Gamma
distributions) allow for analytic posterior calculations of many
quantities of interest. @Klauenberg2015

5.  Bayesian Hierarchical / meta-analytic linear regression and priors
    (exchangeable and unexchangeable)

What is the goal of the paper?

The study developed a test of a formal method for augmenting data in
linear regression analyses, by incorporating both exchangeable and
unexchangeable information on regression coefficients (and standard
errors) of previous studies.

Why is it important?

The frequent combination of multiple testing has relatively low
statistical power, which is problematic in null-hypothesis significance
testing. 
Linear regression analyses do not account for the published
results and summary statistics from similar previous studies. Ignoring information on parameters from previous studies (relevant and readily available), affects the stability and precision of the parameter estimates resulting in lower values than they could have been, resulting in conclusions that are less certain and are affected by sampling variation. 

Multiple linear regression with separate significance tests for all regression coefficients, and with the modest sample sizes, different studies have different sets of statistically significant predictors, and addressing the issue on larger samples is practically unrealistic.

How is it solved?

Bayesian linear regression accommodates prior knowledge.
To overcome the absence of formal studies, it handles the issue of
increasing the sample size, and augments the data of a new study with
regression coefficients and standard errors from previous similar
studies.

To solve the issue of the univariate case analysis, Bayesian linear
regression combines the evidence of specific predictors from different
linear regression analyses (meta-analysis) to resolve the issue of simultaneously combining multiple regression parameters per study, which ignore the relationship between the regression coefficients. 

Adding summary statistics from previous studies in Bayesian linear regression provide a more acceptable solution esp. when previous study data are not (realistically) obtainable.

Based on the information of predictors from previous and current data, the models are categorized into 
(1) Exchangable - when the current data and previous studies have the same set of predictors. 
(2) Unexchangable – when the predictors were different in the two.

The steps to Bayesian linear regression steps are mentioned here that yield the posterior density reflecting the updated knowledge about the model parameters after having observed the data, 

(1) To calculate the probability density function for the data, given
    the unknown model parameters;

(2) The likelihood function - that quantifies what is assumed to
    be known about the model parameters before observing the data. The
    Standard multiple linear regression model, integrate the prior, and
    provide the joint posterior density using the Gibbs sampler.

(3) A hierarchical model version is used to analyze parameters where
    studies are under not-exchangeable category.

Results

Incorporating priors in a linear regression on new data yield a significantly better parameter estimate with an adequate approximation. 
Encouraging performance gains and the large effects are obtained when the data from previous studies are incorporated.
Performance of the two versions (exchangeable and unexchangeable) of the
replication model was consistently superior to using the current data
alone.
The model using exchangeable and unexchangeable prior offers
better parameter estimates in a linear regression setting without the
need to expend a large amount of time and energy to obtain data from the
previous studies.

Hierarchical unexchangeable model version offers the advantage of being
able to address questions about differences between studies and thus
allows for explicit testing of the exchangeability assumption.

Limitations

- All studies need to have the same set of predictors.
- The issue of correlation between predictor variables. @DeLeeuw2012a

6.  Bayesian logistic regression (Bayesian GLM) (Sequential clinical
    reasoning approach)

What is the goal of the paper?

To study was conducted on a longitudinal prospective cohort to develop a model to predict the risk of incident cardiovascular disease. They incorporated (1) demographic features (basic)
(2) six metabolic syndrome components (metabolic score) (3) conventional
risk factors (enhanced model)

The application of Logistic Regression included priors on coefficients
and sequential updating to predict Individual-level CVD risk.

Why is it important?

Early diagnosis of at risk population (CVD), impacts health interventions. Limited availability of molecular information in clinical practice (high cost and unavailability) affects efficient disease diagnosis.

It is required to have an alternative approach to analyze data to
efficiently identify a high-risk population based on the routinely
checked biological markers before doing these expensive molecular tests.

The tailored Framingham Risk Score method, is not sufficient because of
the differences present in ethnic groups, location, and socio-economic
status, and require the construction of their own models. Heterogeneity
(geographic, ethnic group, variations, and different characteristics of
social contextual network) often is unobservable and unmeasurable.

How is it solved?

The subjects enrolled in a screening program (Keelung Community) for mass screening (20–79 years) in
the Keelung city of Taiwan, were analyzed for 5 years to identify
incident cancers and chronic diseases (cardiovascular disease).

The study was able to classify the risk of having incident CVD cases by 
(1) available and calculated standardized risk score of
the MetS components (fasting glucose, blood pressure, HDL-C,
triglyceride and waist circumference) 
(2) together with conventional risk factors (gender, heredity, smoking, alcohol drinking, family
history of parent's CVD and betel quid and other factors).

Emulating a clinician's evaluation process, the Bayesian clinical
reasoning approach in a sequential manner was developed and applied in
three models.

The Bayesian clinical reasoning approach considered the normal distribution of regression
coefficients of all predictors, allowing for uncertainty of clinical
weights. 
The credible intervals of predicted risk estimates were
obtained by averaging out. 
In the model, the individual risk is elicited
by prior speculation (first impression) that is updated by objective
observed data (patient's history and laboratory findings), the
regression coefficients for computing risk score were treated as random
variable with a certain statistical distribution (e.g. normal
distribution) rather than a fixed value (traditional risk prediction
model by frequentist). 
The updated prior distribution with the
likelihood of the current data provided a posterior distribution to
predict the risk for a specific disease. 
The sequential approach included -

1.  Basic model developed via logistic regression used prior information
    constructed on gender, age, age2, and time period.
2.  The Classical model (metabolic score model: MS model) included six
    MetS components.
3.  The third (enhanced model) incorporated information on smoking,
    drinking, betel-quid, and family history of CVD.

Results

Compared to the basic model and classical model, the enhanced model had
better performance. The proposed models predicted CVD incidence at the
individual level by incorporating routine information with a sequential
Bayesian clinical reasoning approach. Patients’ background significantly
contributes to baseline risk. Even with ecological heterogeneity, the
regression model adopts individual characteristics and makes individual
risk prediction for the CVD incidence.

Limitations

- Whether the interactions between age, gender, metabolic score, and
  other risk factors should be included.
- The use of an enhanced model should be validated through external
  validation by applying the proposed models to new subjects not
  included in the training of the model parameters. @Liu2013

7.  What is the goal of the paper?

In this paper, Bayesian parametrization was performed, where the
parameters of the discrete Weibull distribution were conditioned on the
predictors under a uniform non-informative prior, to produce posterior
distribution. The model promises for the wide applicability of to the
analyze count data using R package BDWreg.

Why is it important?

The regression model for a discrete variable based on the discrete
Weibull distribution is a good fit in comparison with other
distributions for count data

The important features of a discrete Weibull distribution that make this
a valuable alternative to the more traditional Poisson and Negative
Binomial distributions and their extensions, such as Poissonmixtures,
Poisson-Tweedie, zero-inflated semiparametric regres- sion and
COMPoisson- the ability to capture both over and under-dispersion and a
closed-form analytical expression of the quantiles of the conditional
distribution.

How is it solved?

They considered non-informative priors, (Jeffreys and uniform), and the
case of Laplace priors with a hyper penalty parameter and proved the
posterior distribution is proper with finite moments under a uniform
non-informative prior.

Results

The advantage of Bayesian approaches over classical maximum likelihood
inference are: (1) the possibility of taking prior information into
account, such as sparsity or information from historical data, (2) the
procedure returns automatically the distribution of all parameters, from
which credible intervals can easily be obtained.

Application

The study compared the proposed model with the Bayesian Poisson
(BPoisson), Bayesian Negative Binomial (BNB) models and Bayesian DW
model on three datasewts (inhaler use, health survey, health registry),
where BDW(regQ,β) models showed superior performance to the other
models. The Bayesian discrete Weibull model sjows applicability in
analysing count data from the medical domain.

The analysis was applied on datasets (number of visits to
doctors/specialists, - an indicator of healthcare demand) with discrete
response variable and a skewed distribution,under-dispersion,
over-dispersion and excess of zeros.

8. Diabetes prediction

In a case–control study, they assessed the association between body mass index (BMI) and the risk of having T2D in adults diagnosed with T2D (from an electronic health records database) and comapred with double count of controls. 

The effects of age, sex, cardiac comorbidities/ C-reactive protein and erythrocyte sedimentation rate, and medications on a T2D diagnosis were on odds ratios (OR) and relative risks (RR) estimated from multiple logistic regression results. Impact of baseline BMI on the risk of T2D diagnosis reported odds ratios (ORs) estimated from an unconditional multiple logistic regression model that adjusted for other covariates. They also estimated the adjusted relative risk of T2D diagnosis for each BMI category (with normal BMI as the comparator) using the method of recycled predictions. 

The risk of T2D increasingly larger for individuals in higher BMI categories than for individuals in lower BMI categories. Even after adjusting for confounding factors, the potential for unmeasured, and unadjusted, confounding in baseline characteristics was not accounted in the analysis. 

This approach, introduces substantial bias since there is no evidence that the likelihood for individuals to seek care outside of the health system was correlated with T2D or obesity. They mentioned availability of data to account for individuals’ pre-diabetic status and the timing and duration of obesity. The problem is managed by Bayesian Methods (e.g., Bayesian hierarchical models, Bayesian propensity scores) with flexibility to incorporate prior knowledge. Useful when the sample size is limited, or you want uncertainty in priors to inform the model. @M.L.2014



## Methods


*The common non-parametric regression model is*
$Y_i = m(X_i) + \varepsilon_i$*, where* $Y_i$ *can be defined as the sum
of the regression function value* $m(x)$ *for* $X_i$*. Here* $m(x)$ *is
unknown and* $\varepsilon_i$ *some errors. With the help of this
definition, we can create the estimation for local averaging i.e.*
$m(x)$ *can be estimated with the product of* $Y_i$ *average and* $X_i$
*is near to* $x$*. In other words, this means that we are discovering
the line through the data points with the help of surrounding data
points. The estimation formula is printed below [@R-base]:*

## Method and data preparation

Methodology

Statistical software R is used, packages installed and libraries were
run in R for data import, management and analysis. 
Using haven
package .XPT files were converted to R-object as dataframe (df).

Statistical Method Used:

Bayesian logistic Regression (GLM) is used to calculate the
probability of Diabetes. We incorporate default Prior for Bayesian logistic for
Diabetes (Binary response variable), using Bernoulli likelihood
with a logit link is the standard, interpretable model for risk b.

1. By adding Priors -
• Shrinkage is possible when including predictors (BMI, age, gender, race ). 
• Clinical beliefs (e.g., positive association for
age/BMI, protective for HDL) can be encoded as priors to stabilize
estimates and improve calibration 

2. Bayesian modeling plays nicely with multiple imputation or joint, propagating uncertainty instead of
silently dropping cases. 

3. Calculated Posterior and credible intervals for both coefficients and individual risk make clinical
interpretation and shared decision-making models. 

4. Bayesian logistic GLM gives an interpretable, survey-aware, uncertainty-quantified Diabetes
risk model that’s robust to sparse subgroups and easy to update. 

5. It is easy for an update, a new NHANES cycle can update the calculated posterior
rather than refitting from scratch, developing a live risk model.

Baysian MI and Bayesian Logistic regression @Austin2021

$$
M_n(x) = \sum_{i=1}^{n} W_n (X_i) Y_i  \tag{1}
$$$W_n(x)$ *is the sum of weights that belongs to all real numbers.
Weights are positive numbers and small if* $X_i$ *is far from* $x$*.*

*Another equation:*

$$
y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i
$$

```{r}
#| label: Libraries
#| include: false


# loading packages 

library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(Hmisc)
library(dplyr)
library(tidyr)
library(forcats)
library(ggplot2)
library ("nhanesA")                 

options(repos = c(CRAN = "https://cloud.r-project.org"))

install.packages("nhanesA")

```

## Analysis and Results

## Data Exploration and Visualization

Data Source

NHANES 2-year cross-sectional data, with all variables measured at the
same visit was extracted from 5 datasets (demographics, exam,
questionnaire) from year 2013-2014.@CenterforHealthStatistics1999

Data Management

The download datsets were explored for variables of
interest. Selected variables were merged into a dataframe and later to create subsets for analysis. 
The subsets were then merged to an analytic dataframe, cleaned, categorized, and recoded. 
The merged dataframe was then explored for basic statistics and any anamolies and patters before running regression analysis.

To have a final dataset is ready for interpretation, has good statistical
power. the descriotives of and proper handling of missing values was conducted. Final Analytic dataset included binary response variable (Diabetes), predictor variables (BMI), and covariates (age, gender, and race). Multiple
linear regression is performed and to handle complete-case deletion issues (liswise deletion) the quasi-complete data was handled by Baysian data augmentatio before running Bayesian logistic regression. 


```{r message=FALSE, warning=FALSE}
#| label: nhanes tables
#| include: false

 # making subsets for each dataset  
                       
                       nhanesTables('EXAM', 2013)
                       nhanesTables('QUESTIONNAIRE', 2013)
                       nhanesTables('DEMOGRAPHICS', 2013)
                       
                  
  #  files for 2013–2014
                       
                         bmx_h <- nhanes("BMX_H")         #Exam
                       smq_h <- nhanes("SMQ_H")         #Quest
                       demo_h <- nhanes("DEMO_H")       #Demo
                       diq_h <- nhanes("DIQ_H")         #diabetes


# Example: pick only the variables you need
exam_sub <-   bmx_h  %>% select(SEQN, BMDBMIC)
demo_sub <- demo_h %>% select(SEQN, RIDAGEYR, RIAGENDR, RIDRETH1, SDMVPSU, SDMVSTRA, WTMEC2YR)
diq_sub <-  diq_h %>% select (SEQN, DIQ240)


names(exam_sub)
names(demo_sub)
names(diq_sub)


merged_data <- exam_sub %>%
  left_join(demo_sub, by = "SEQN") %>%
  left_join(diq_sub, by = "SEQN")
head(merged_data)

                       
                       
library(survey)
nhanes_design <- svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  data = merged_data,
  nest = TRUE
)

# Example: weighted mean age
svymean(~RIDAGEYR, design = nhanes_design, na.rm = TRUE)
svymean(~SEQN, design = nhanes_design, na.rm = TRUE)
svymean(~BMDBMIC, design = nhanes_design, na.rm = TRUE)
svymean(~RIAGENDR, design = nhanes_design, na.rm = TRUE)
svymean(~RIDRETH1, design = nhanes_design, na.rm = TRUE)


                       
```

Data Preparation

Variable Selection and merging of the subsets

Subsets were created from each downloaded dataframe with selected
variables of interest, and explored for NAs. Subsets were then merged to
form a combined dataframe for analysis.

```{r message=FALSE, warning=FALSE}
#| label: merged dataframe
#| include: false

unique(diq_h$DIQ240)
table(diq_h$DIQ240, useNA = "ifany")

unique(demo_h$RIAGENDR)
table(demo_h$RIAGENDR, useNA = "ifany")

unique(demo_h$RIDAGEYR)
table(demo_h$RIDAGEYR, useNA = "ifany")

unique(demo_h$RIDRETH1)
table(demo_h$RIDRETH1, useNA = "ifany")

unique(bmx_h$BMDBMIC)
table(bmx_h$BMDBMIC, useNA = "ifany")

```

Data Cleaning

• Silently dropping cases = throwing out any rows with missing values
(aka complete-case analysis). Loosing sample size (power) can introduce
bias if the missingness isn’t completely random. In the dataset, the
missing values (NA) are kept as NA, and we used complete-case analysis
(or “listwise deletion”) criteria as regression with glm(), R will
automatically drop any row with NA in the model variables. It was the
simplest approach to handle missingness, and we aware of losing sample
size and introducing bias if missingness is informative.

• NHANES special codes (as below) were transformed to NAs, preserving
the number of rows intact, considering special codes are not random and
cannot be dropped. Type Codes / Values Meaning Refused 7, 77, 777, 7777
Participant refused to answer Don’t know / Unknown 9, 99, 999, 9999
Participant didn’t know or unknown Not applicable / skipped 8, 88, 888,
8888 Variable not applicable or skipped Other 0 Sometimes “No” depending
on variable (check codebook)


```{r message=FALSE, warning=FALSE}
#| label: missing values_final data
#| include: false


sum(is.na(merged_data$DIQ240))   # number of missing in DIQ240
sum(is.na(merged_data$RIDAGEYR)) # number of missing in RIDAGEYR
sum(is.na(merged_data$BMDBMIC)) # number of missing in RIDAGEYR
sum(is.na(merged_data$RIAGENDR)) # number of missing in RIDAGEYR
sum(is.na(merged_data$SEQN)) # number of missing in RIDAGEYR


# Count NAs in all columns
colSums(is.na(merged_data))

missing_codes <- c(7, 8, 9, 77, 88, 99, 777,999)
# Example for DIQ240
sum(merged_data$DIQ240 %in% missing_codes)
sum(merged_data$RIDAGEYR %in% missing_codes)
sum(merged_data$BMDBMIC %in% missing_codes)
sum(merged_data$RIAGENDR  %in% missing_codes)
sum(merged_data$SEQN  %in% missing_codes)


merged_data # 9813 obs with 9 variables 

# cleaning merged_data (# remove NAs)

clean_data <- merged_data %>% ## filtering ##  row filter listwise removal
  
  filter(
    !is.na(DIQ240),
    !is.na(RIDAGEYR),
    !is.na(BMDBMIC),
    !is.na(RIAGENDR),
    !is.na(SEQN)
     )


missing_codes <- c(7, 8, 9, 77, 88, 99, 777,999)     # checking special codes

sum(clean_data$DIQ240 %in% missing_codes)
sum(clean_data$RIDAGEYR %in% missing_codes)
sum(clean_data$BMDBMIC %in% missing_codes)
sum(clean_data$RIAGENDR  %in% missing_codes)


clean_data            # clean_data with no NAs (14 obs and 9 variables)

summary(clean_data)

```

Creating new variables and categorization

1.  Response Variable

    Diabetes - defined as "Is there one Dr you see for diabetes"
    Predictor Variables

      -   Body Mass Index (BMI)

        The original data has BMDBMIC (measured BMI) as categorical and
        had no missing values. It (BMI_cat) has the following 4 levels:\
        o Underweight (\<5th percentile)\
        o Normal (5th–\<85th)\
        o Overweight (85th–\<95th) o Obese (≥95th percentile)\
        o Missing We kept it as it is as categorization provides
        clinically interpretable groups.

    Covariates:

    -   Gender (factor, 2 levels): Male: Female
    -   Ethnicity (factor, 5 levels): Mexican American, Non-Hispanic,
        White Non-Hispanic, Black Other Hispanic\
        Other Race - Including Multi-Racial


```{r}
#| label: new columns
#| include: false
 
 # After listwise deletion of NAs, the clean_data - not usable data #)

# cleaning of special characters(7,9,77,99) from merged_data (Raw data)

special_codes <- c(7, 9, 77, 99)  # removal of special codes from merged data

cols_to_clean <- c("SEQN", "DIQ240","BMDBMIC","RIDAGEYR","RIAGENDR", "SDMVPSU","SDMVSTRA", "WTMEC2YR")

# Loop over columns

for (v in cols_to_clean) {
  merged_data[[v]][merged_data[[v]] %in% special_codes] <- NA
}

summary(merged_data)  ## no removal of NAs in merged_data 
                      # NAs are handled in the model 


table (merged_data$BMDBMIC)
table (merged_data$DIQ240)
table (merged_data$RIAGENDR)
table (merged_data$RIDRETH1)
table (merged_data$RIDAGEYR)
table(merged_data$BMDBMIC, merged_data$DIQ240)  # Listwise deletion - only 14 rows match
table(merged_data$RIDRETH1, merged_data$DIQ240)

summary (merged_data)     # NAs in RIDAGEYR,DIQ240,BMDBMIC

# Multiple Logistic Regression with missingness

m1<- glm(DIQ240 ~ BMDBMIC + RIDAGEYR + RIAGENDR + RIDRETH1,
                       family = binomial, data = merged_data)  # quasi-complete separation

# planning for Bayesian data augmentation

# MCMC =  samples from the joint posterior (both regression coefficients and the missing values).

```

-   Initial findings and insights through visualizations.
-   Summary statistics, histogram (continuous variables), and bar plot
    (categorical variables) were observed before regression analysis

```{r message=FALSE, warning=FALSE}
#| label: dat results and tables
#| include: false



## Imputation because the merged data has less number of complete cases (missingness in BMI and diabetes variables)

library(mice)


# Subset variables for imputation in analytic_data df

vars <- c("SEQN","BMDBMIC",  "RIDAGEYR", "RIAGENDR", "RIDRETH1", "SDMVPSU",  "SDMVSTRA", "WTMEC2YR", "DIQ240"  )
analytic_data <- merged_data[, vars]


# Run mice to create 5 imputed datasets

imputed_data <- mice(
  analytic_data,
  m = 5,              # number of imputed datasets
  method = 'pmm',     # predictive mean matching
  seed = 123
)


# First imputed dataset

Imputed_data1 <- complete(imputed_data, 1)

# Check if missingness is gone

summary(Imputed_data1)
colSums(is.na(Imputed_data1))  # check total missing values


```

Unexpected reports, patterns or anomalies.

-   The age showed a skewed histogram. Most population were from
    <20years range followed by 40-59years range with mean age being 26
    years
-   Non-Hispanic White were majority in the study cohort.
-   Majority reported normal weight but followed by higher number of
    obese population.
-   Female: Male population reported a higher side on females.
-   Total number of population being told having Diabetes reported "NO"

```{python}
import pandas as pd






```

```{r warning=FALSE}
#| label: Modeling ansd Results
#| echo: true



merged_data <- rename(merged_data,
       id = SEQN, # Respondent sequence number
       gender = RIAGENDR,            # Gender
       age = RIDAGEYR,               # Age in years at screening
       diab = DIQ240,                # Do you see a dr for diabetes
       BMI_cat = BMDBMIC,            # BMI
       race = RIDRETH1               #Race
       )
head(merged_data,5)

# Numeric summary for age
summary(merged_data$age)
summary(merged_data$gender)
summary(merged_data$race)
summary(merged_data$BMI_cat)
summary(merged_data$diab)


# Counts for categorical variables
table(merged_data$age)
table(merged_data$gender)
table(merged_data$BMI_cat)
table(merged_data$diab)
table(merged_data$race)

# Proportions
prop.table(table(merged_data$diab))
prop.table(table(merged_data$BMI_cat))
prop.table(table(merged_data$gender))
prop.table(table(merged_data$age))
prop.table(table(merged_data$race))


colSums(is.na(merged_data))

table(merged_data$diab, merged_data$BMI_cat)
table(merged_data$BMI_cat, merged_data$diab, useNA="ifany") # before cleaning and imputation
table(merged_data$gender, merged_data$diab, useNA="ifany")
table(merged_data$age, merged_data$diab, useNA="ifany")

  

```

### Modeling and Results

-   Explain your data preprocessing and cleaning steps.

-   Present your key findings in a clear and concise manner.

-   Use visuals to support your claims.

-   **Tell a story about what the data reveals.**

```{r}


library(ggplot2)

# Age distribution
ggplot(merged_data, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Age Distribution", x = "Age (years)", y = "Count")

# BMI category
ggplot(merged_data, aes(x = BMI_cat)) +
  geom_bar(fill = "salmon") +
  theme_minimal() +
  labs(title = "BMI Category Distribution", x = "BMI Category", y = "Count")

# Race category
ggplot(merged_data, aes(x = race)) +
  geom_bar(fill = "salmon") +
  theme_minimal() +
  labs(title = "Race Category Distribution", x = "Race Category", y = "Count")

# Gender
ggplot(merged_data, aes(x = gender)) +
  geom_bar(fill = "lightgreen") +
  theme_minimal() +
  labs(title = "Gender Distribution", x = "Gender", y = "Count")

# Outcome
ggplot(merged_data, aes(x = factor(diab))) +
  geom_bar(fill = "purple") +
  theme_minimal() +
  labs(title = "Diabetes Status (DIQ240)", x = "Diabetes (2 = No, 1 = Yes)", y = "Count")


# Cross - tabulation #

# DIQ240 by Age #
ggplot(merged_data, aes(x = age , y = factor(diab))) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  labs(title = "Age by Diabetes Status", x = "Age", y = "Diabetes (2 = No, 1 = Yes)" )


#  DIQ240 by BMI  #

## ALL NAs ## 
ggplot(merged_data, aes(x = BMI_cat  , fill = factor(diab))) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Diabetes by BMI Category", x = "BMI Category", y = "Proportion", fill = "diab")   


# DIQ240 by Race  #
ggplot(merged_data, aes(x = race, fill = factor(diab)  )) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Diabetes by Race ", x = "Race ", y = "Proportion", fill = "diab")

# DIQ240 by Gender

ggplot(merged_data, aes(x = gender, fill = factor(diab)  )) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Diabetes by Race ", x = "gender ", y = "Proportion", fill = "diab")


```




```{r}

# load a libraries
library(knitr) # fancy table
library(tidyverse) # load library tidyverse
library(classpackage)


# To display fancy tables
kable(head(merged_data,10))
table(merged_data$diab, useNA = "ifany")

# modelling assuption check (# for correlation # )

# 1. complete case dataset (assumotions)

names(Imputed_data1)

m1 <- glm(DIQ240 ~ RIDAGEYR + RIAGENDR + RIDRETH1 + BMDBMIC,
          data = Imputed_data1, 
          family = binomial)

# Get the predicted logit
Imputed_data1$logit <- predict(m1, type = "link")  # 'link' gives log-odds

# Plot BMI vs logit

plot1 <- ggplot(Imputed_data1, aes(x = RIDAGEYR, y = logit)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess") +
  labs(x = "RIDAGEYR", y = "Log-odds of Diabetes")

plot1       #Implication for modeling:

# Relationship (age and log-odds of diabetes)= roughly linear but not perfectly, acceptable for logistic regression assumptions ##


# collinearity check #

library(car)
vif(m1)  # VIF > 5 or 10 indicates multicollinearity # No multicollinearity # 


# Check for outliers # 
library(broom)
influence <- broom::augment(m1)
ggplot(influence, aes(x = .hat, y = .cooksd)) +
  geom_point() +
  labs(x = "Leverage", y = "Cook's Distance")



Imputed_data1 <- Imputed_data1 %>%
  mutate(outlier =  if_else(abs(rstandard(m1))>2.5, "Suspected", "Not Suspected"))


Imputed_data1 %>% count(outlier)



Imputed_data1 %>% ggplot(aes(x = RIDAGEYR, y = DIQ240, color = outlier)) +
  geom_point() + 
  scale_color_manual(values = c("#999999", "#000000")) +
  labs(x = "RIDAGEYR", y = "DIQ240", color = "Outlier") +
  theme_bw()


library(ResourceSelection)
hoslem.test(Imputed_data1$DIQ240, fitted(m1))

library(pROC)
roc(Imputed_data1$DIQ240, fitted(m1)) %>% plot()



anova_check(m1)
summary(m1)

library(glmtoolbox)
(adjR2(m1))


levels(Imputed_data1$DIQ240)
# Output: "No" "Yes"

# Table of counts for each code
table(Imputed_data1$DIQ240, useNA = "ifany")


labelled::look_for(Imputed_data1, "DIQ240")

# Assuming your data frame is named 'df' and the column is 'my_binary_column'
table(Imputed_data1$DIQ240) 

table(diq_h$DIQ240)
unique(diq_h$DIQ240)

str(diq_h$DIQ240)
# See numeric codes (1 = Yes, 2 = No)
table(as.numeric(Imputed_data1$DIQ240), useNA = "ifany")
```


```{r}
#| label: brms
#| eval: false
#| include: false


glm(DIQ240     ~ RIDAGEYR + BMDBMIC + RIDRETH1 + RIAGENDR                                   , 
    family = binomial, data = Imputed_data1)

names(merged_data)

library(brms)

# Main logistic regression model with missing-data handling
form_main <- bf(
  diab ~ mi(BMI_cat) + mi(age) + gender + race,
  family = bernoulli()
)

# Submodels for missing variables

form_age <- bf(age | mi() ~ gender + gender + BMI_cat)
form_bmi <- bf(BMI_cat | mi() ~ gender + race + age)

# Combine into one joint model
form_all <- form_main + form_age + form_bmi

# Fit Bayesian logistic regression with data augmentation


get_prior(form_all, data = merged_data, family = bernoulli())






fit_bayes <- brm(
  formula =form_all,
  data = merged_data,
  prior = c(
    set_prior("normal(0, 2.5)", class = "b")  # weakly informative prior
  ),
  chains = 4, iter = 2000, cores = 4, seed = 123
)

summary(fit_bayes)
```



### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
