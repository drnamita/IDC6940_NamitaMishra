---
title: "Bayesian Logistic Regression models and missingness - Application in Diabetes Probability Prediction"
subtitle: "CapStone Project_2025"
author: "Namita Mishra (Advisor: Dr. Ashraf Cohen)"
date: '`r Sys.Date()`'
format:
  html:
    code-fold: true
course: Capstone Projects in Data Science
bibliography: references.bib # file contains bibtex for references
#always_allow_html: true # this allows to get PDF with HTML features
self-contained: true
execute: 
  warning: false
  message: false
editor: 
  markdown: 
    wrap: 72
---

## Literature Review

## Introduction

1.  **Bayesian Hierarchical Model and MCMC**

What is the goal of the paper?

To develop a Bayesian hierarchical model for multivariate longitudinal
data to predict health status, trajectories, and intervention effects at
the individual level in PCORI mission.

Why is it important?

Healthcare data include DNA sequences, functional images of the brain,
patient-reported outcomes, and electronic health records with patients’
sequences of health measurements, diagnoses, and treatments. are
complex, and the standard approaches are not adequate for clinical
datasets.

How is it solved?

Use of Electronic Health Records (EHRs) with improved the diagnostic
accuracy and predicts treatment effects. Visualizations of posterior
distributions to help clinicians and patients to make their decision.
Combining prior knowledge and patient data with evidence predicted the
patient’s health status, trajectory, and/or likely benefits of
intervention.

Use of Bayesian hierarchical regression on multivariate longitudinal
patient data (R-packages) developed two-levels (1)time within person and
(2) persons within a population along with co-variates and
interventions. They combined exogenous (eg, age, clinical history) and
endogenous (eg, current treatment) variables on the individual’s
multivariate health measurements, with the effects of health
measurements at one time on subsequent interventions.

Bayesian hierarchical model provided posterior distribution for
predictor variables and an estimate of the marginal distribution of the
regression coefficients for each coefficient. A large sample which is
based on likelihood dominates the prior distribution for regression
coefficients. Bayesian hierarchical model is a likelihood-based approach
and uses priors providing sensitivities.

The integration of Markov chain Monte Carlo (MCMC) estimated the
posterior distributions, and avoided missing data and complex outcome
measurements.

Results and application

-   Applied in pneumonia etiology (children), prostate cancer, and
    mental disorders to identify low-risk patient population
-   To reduce the risk of over-treatment, complications,adverse effects,
    and financial burden for patients (Disease Reclassification).
-   Prostate cancer software implementation within the hospital setup
    (JHM HER).

Limitation:

Models were entirely parametric, and recommended for extensions to
nonparametric or more flexible parametric models for neuroimage or
genomic data.

Recommendations are to address unmet need across a larger, diverse
population, in other diseases (autoimmune diseases, sudden cardiac
arrest, and diabetes) and to embed tools to acquire and use the most
relevant information for a better outcome at an affordable cost.
@Zeger2020

2.  **Bayesian Inference (parametric vs non-parametric)**

What is the goal of the paper?

The study calculated the posterior probability of disease diagnosis by
applying Bayesian inference to develop two modules comparing parametric
(with a fixed set of parameters) and nonparametric distributions (which
do not make a priori assumptions). The National Health and Nutrition
Examination Survey data from two separate diagnostic tests on both
diseased and non-diseased populations were used for model development.

Why is it important?

Conventional methods based on clinical criteria and fixed numerical
thresholds limit the information captured on the intricate relationship
between diagnostic tests and the varying prevalence of diseases. The
probability distributions associated with quantitative diagnostic test
outcomes have some overlap between the diseased and nondiseased groups.
The dichotomous method fails to capture the complexity and heterogeneity
of disease presentations across diverse populations. The applicability
of the normal distribution (conventional method) is critiqued in dealing
with skewness, bimodality, or multimodality.

How is it solved?

Bayesian nonparametric (vs parametric) diagnostic modeling is a Flexible
distributional modeling for test outcomes; posterior disease
probabilities

The study developed models using Bayesian inference for posterior
probability calculation in the Wolfram Language by integrating prior
probabilities of disease with distributions in both diseased and
nondiseased populations. The approach enabled the evaluation of combined
data from multiple diagnostic tests and resulted in improved the
diagnostic accuracy, precision, and adaptability. The model showed
flexibility, adaptability, and versatility in the diagnostic.

Results

Nonparametric Bayesian models were reported a better fit for data
distributions given the limited existing literature, and were robust in
capturing complex data patterns, producing multimodal probability
patterns for disease, unlike the bimodal, double-sigmoidal curves seen
with parametric models.

Limitations

-   Reliance on parametric models

-   Limited scholarly publications and over-dependence on prior
    probabilities increase the uncertainties, resulting in broader
    confidence intervals for posterior probabilities.

-   Systemic bias (unrepresentative datasets) compromises the accuracy
    of Bayesian calculations.

-   For Incomplete datasets, Bayesian methods combined with other
    statistical and computational techniques could enhance diagnostic
    capabilities.

-   Absence of normative data compromises the reliability and validity
    of Bayesian diagnostic methods. @Chatzimichail2023

3.  **Bayesian methodology overview (stages, development and
    advantages)**

What is the goal of the paper?

The stages of Bayesian analysis are presented here specifying the
importance of the priors, data modeling, inferences, model checking and
refinement, selecting a proper sampling technique from a posterior
distribution, variational inferences, variable selection, and its
application across various research fields.

Why is it important?

Bayesian statistics is used across different fields (social sciences,
ecology, genetics, medicine) where observed and unobserved parameters
exist.

Variable selection- is the process of identifying the sub-set of
predictors to include in a model along with determining the functional
form of the model especially where a large number of potential
predictors are available. Unnecessary variables in a model are
associated with issues such as multicollinearity, insufficient samples,
overfitting the current data leading to poor predictive performance on
new data and making model interpretation more difficult.

How is it solved?

Variables selection is best after checking correlations among the
variables in the model (Eg: gene-to-gene interaction to predict genes in
biomedical research).

When the sample size is small, Bayesian estimation with mildly
informative priors is often used. The study describes categories of
priors (informative, weakly informative and diffuse) based on the degree
of (un)certainty (hyperparameters) surrounding the population parameter.
The prior distribution with a larger variance represents a greater
amount of uncertainty surrounding. Prior elicitation through experts,
generic expert, data-based, sample data using maximum likelihood or
sample statistics, etc help construct a prior distribution. A prior
sensitivity analysis of the likelihood - examines different forms of the
model and assesses how the priors and the likelihood align and impact on
posterior estimates, reflecting variations not captured by the prior or
the likelihood alone. Prior estimation -allows data-informed shrinkage,
regularization or influence algorithms towards a likely high-density
region, and improves estimation efficiency.

In a small sample i.e. less information, incorporation of priors
strengthens the observed data and lends possible value(s) for the
unknown parameter(s). Knowing the exact probabilistic specification of
the priors for a complex model with smaller sample sizes is important.
Frequentists do not consider the probability of the unknown parameters
as useful, and consider to be fixed; likelihood is considered as the the
conditional probability distribution p(y\|θ) of the data (y), given
fixed parameters (θ).

In Bayesian inference, unknown parameters (random variables) have varied
values, while the (observed) data have fixed values. The likelihood is a
function of θ for the fixed data y. Therefore, the likelihood function
summarizes a statistical model that stochastically generates a range of
possible values for θ and the observed data y. With priors and the
likelihood of the observed data, the resulting posterior distribution
provides an estimate of the unknown parameters, capture primary factors
to improve our understanding. Monte Carlo technique provides integrals
of sampled values from a given distribution through computer
simulations. The packages BRMS and Blavaan in R are used for the
probabilistic programming language Stan. MCMC algorithm only requires
the probability distribution of interest to be specified up to a
constant of proportionality and is scalable to high dimensions to obtain
empirical estimates of the posterior distribution of interest. Bayesian
inference adopts a simulation-based strategy for approximating posterior
distributions.

Spatial and temporal variability are factored in Bayesian general linear
models. A posterior distribution can simulate new data conditional on
this distribution and assess, to providing valid predictions.

Results and application

The Bayesian approach analyzes large-scale cancer genomic data,
identifies novel molecular changes in cancer initiation and progression,
the interactions between mutated genes and captured mutational
signatures, highlighting key genetic interactions components, allowing
genomic-based patient stratification both in clinical trials, in the
personalized use of therapeutics, and in understanding cancer and its
evolutionary processes.

The study proposes strategies for reproducibility and reporting
standards, outlining a checklist and emphasize on the impact of Bayesian
analysis on artificial intelligence in the future.

Limitations

In temporal models, the spatial and/or temporal dependencies
(autocorrelation of parameters over time)is a challenge in posterior
inference.The subjectivity of priors is highlighted by critics as a
potential drawback of Bayesian methods. @VandeSchoot2021

4.  **Bayesian Normal linear regression, Core parametric (conjugate)
    model with Normal–Inverse-Gamma prior**

What is the goal of the paper?

The study emphasizes prior elicitation, analytical posteriors,
robustness checks through guidance provided on Bayesian inference by
performing Bayesian Normal linear regression in metrology to calibrate
instruments to evaluate inter-laboratory comparisons in determining
fundamental constants.

Why is it important?

Errors are **independent and identically distributed in Gaussian** and
variance is unknown and is estimated from data, the relationship between
X and Y is statistical, with noise and model uncertainty and the
regression can not be treated as a measurement function . There is a
need for statistical approaches (likelihood, Bayesian, bootstrap, etc.)
to quantify uncertainty Guide to the Expression of Uncertainty in
Measurement (GUM) and its supplements are not applicable directly.

How is it solved?

Bayesian inference accounts for a priori information, and robustifies
the analyses. It emphasizes steps (prior elicitation, posterior
calculation, and robustness to prior uncertainty and model adequacy) for
the model development and about assumptions critical to Bayesian
inference.

All unknowns (observables (data) and unobservables such as parameters
and auxiliary variables) are considered random, are assigned probability
distributions of the available information, and update prior knowledge
about the unobservables with information about them contained in the
data. The graphical representation of prior distribution and likelihood
function, sensitivity analyses, or model checking enhances the
elicitation and interpretation process.

For Normal linear regression problems (1) prior distribution - Normal
inverse Gamma (NIG) distribution to a posterior is from the same family
of (NIG) distribution. The NIG prior with known variance σ2 of
observations is a conjugate prior distribution. Vague or non-informative
prior distributions can be derived from NIG prior. (2) alternative
families of prior distributions (hierarchical priors) assign an
additional layer of distributions to uncertain prior parameters or
non-parametricriors.

Bayesian inference is influenced by

\- the uncertainty in the transformation of prior knowledge to prior
distributions

\- the assumptions of the statistical model

\- the mistakes in data acquisition

Results and Application

The knowledge from related previous experiments (Normal inverse Gamma
distributions) allows for analytic posterior calculations of many
quantities of interest. @Klauenberg2015

5.  **Bayesian Hierarchical / meta-analytic linear regression and priors
    (exchangeable and unexchangeable)**

What is the goal of the paper?

The study developed a test of a formal method for augmenting data in
linear regression analyses, by incorporating both exchangeable and
unexchangeable information on regression coefficients (and standard
errors) of previous studies.

Why is it important?

The frequent combination of multiple testing has relatively low
statistical power, which is problematic in null-hypothesis significance
testing. Linear regression analyses do not account for the published
results and summary statistics from similar previous studies. Ignoring
information on parameters from previous studies (relevant and readily
available), affects the stability and precision of the parameter
estimates resulting in lower values than they could have been, resulting
in conclusions that are less certain and are affected by sampling
variation.

Multiple linear regression with separate significance tests for all
regression coefficients, and with the modest sample sizes, different
studies have different sets of statistically significant predictors, and
addressing the issue on larger samples is practically unrealistic.

How is it solved?

Bayesian linear regression accommodates prior knowledge. To overcome the
absence of formal studies, it handles the issue of increasing the sample
size, and augments the data of a new study with regression coefficients
and standard errors from previous similar studies.

To solve the issue of the univariate case analysis, Bayesian linear
regression combines the evidence of specific predictors from different
linear regression analyses (meta-analysis) to resolve the issue of
simultaneously combining multiple regression parameters per study, which
ignore the relationship between the regression coefficients.

Adding summary statistics from previous studies in Bayesian linear
regression provide a more acceptable solution esp. when previous study
data are not (realistically) obtainable.

Based on the information of predictors from previous and current data,
the models are categorized into (1) Exchangable - when the current data
and previous studies have the same set of predictors. (2) Unexchangable
– when the predictors were different in the two.

The steps to Bayesian linear regression steps are mentioned here that
yield the posterior density reflecting the updated knowledge about the
model parameters after having observed the data,

(1) To calculate the probability density function for the data, given
    the unknown model parameters;

(2) The likelihood function - that quantifies what is assumed to be
    known about the model parameters before observing the data. The
    Standard multiple linear regression model, integrate the prior, and
    provide the joint posterior density using the Gibbs sampler.

(3) A hierarchical model version is used to analyze parameters where
    studies are under not-exchangeable category.

Results

Incorporating priors in a linear regression on new data yield a
significantly better parameter estimate with an adequate approximation.
Encouraging performance gains and the large effects are obtained when
the data from previous studies are incorporated. Performance of the two
versions (exchangeable and unexchangeable) of the replication model was
consistently superior to using the current data alone. The model using
exchangeable and unexchangeable prior offers better parameter estimates
in a linear regression setting without the need to expend a large amount
of time and energy to obtain data from the previous studies.

Hierarchical unexchangeable model version offers the advantage of being
able to address questions about differences between studies and thus
allows for explicit testing of the exchangeability assumption.

Limitations

-   All studies need to have the same set of predictors.
-   The issue of correlation between predictor variables. @DeLeeuw2012a

6.  **Bayesian logistic regression (Bayesian GLM) (Sequential clinical
    reasoning approach)**

What is the goal of the paper?

To study was conducted on a longitudinal prospective cohort to develop a
model to predict the risk of incident cardiovascular disease. They
incorporated (1) demographic features (basic) (2) six metabolic syndrome
components (metabolic score) (3) conventional risk factors (enhanced
model)

The application of Logistic Regression included priors on coefficients
and sequential updating to predict Individual-level CVD risk.

Why is it important?

Early diagnosis of at risk population (CVD), impacts health
interventions. Limited availability of molecular information in clinical
practice (high cost and unavailability) affects efficient disease
diagnosis.

It is required to have an alternative approach to analyze data to
efficiently identify a high-risk population based on the routinely
checked biological markers before doing these expensive molecular tests.

The tailored Framingham Risk Score method, is not sufficient because of
the differences present in ethnic groups, location, and socio-economic
status, and require the construction of their own models. Heterogeneity
(geographic, ethnic group, variations, and different characteristics of
social contextual network) often is unobservable and unmeasurable.

How is it solved?

The subjects enrolled in a screening program (Keelung Community) for
mass screening (20–79 years) in the Keelung city of Taiwan, were
analyzed for 5 years to identify incident cancers and chronic diseases
(cardiovascular disease).

The study was able to classify the risk of having incident CVD cases by
(1) available and calculated standardized risk score of the MetS
components (fasting glucose, blood pressure, HDL-C, triglyceride and
waist circumference) (2) together with conventional risk factors
(gender, heredity, smoking, alcohol drinking, family history of parent's
CVD and betel quid and other factors).

Emulating a clinician's evaluation process, the Bayesian clinical
reasoning approach in a sequential manner was developed and applied in
three models.

The Bayesian clinical reasoning approach considered the normal
distribution of regression coefficients of all predictors, allowing for
uncertainty of clinical weights. The credible intervals of predicted
risk estimates were obtained by averaging out. In the model, the
individual risk is elicited by prior speculation (first impression) that
is updated by objective observed data (patient's history and laboratory
findings), the regression coefficients for computing risk score were
treated as random variable with a certain statistical distribution (e.g.
normal distribution) rather than a fixed value (traditional risk
prediction model by frequentist). The updated prior distribution with
the likelihood of the current data provided a posterior distribution to
predict the risk for a specific disease. The sequential approach
included -

1.  Basic model developed via logistic regression used prior information
    constructed on gender, age, age2, and time period.
2.  The Classical model (metabolic score model: MS model) included six
    MetS components.
3.  The third (enhanced model) incorporated information on smoking,
    drinking, betel-quid, and family history of CVD.

Results

Compared to the basic model and classical model, the enhanced model had
better performance. The proposed models predicted CVD incidence at the
individual level by incorporating routine information with a sequential
Bayesian clinical reasoning approach. Patients’ background significantly
contributes to baseline risk. Even with ecological heterogeneity, the
regression model adopts individual characteristics and makes individual
risk prediction for the CVD incidence.

Limitations

-   Whether the interactions between age, gender, metabolic score, and
    other risk factors should be included.
-   The use of an enhanced model should be validated through external
    validation by applying the proposed models to new subjects not
    included in the training of the model parameters. @Liu2013

7.  **Bayesian parameter estimation in discrete Weibull regression**

What is the goal of the paper?

The study provided the Bayesian approach for parameter estimation in
discrete Weibull regression. Bayesian parametrization performed where
both on parameters of the discrete Weibull distribution can be
conditioned on the predictors under a uniform non-informative prior, to
produce posterior distribution. The model promises its wide
applicability to analyze count data using R package BDWreg.

Why is it important?

Discrete data (eg: quality of care, planning capacity within a hospital,
the number of visits to a specialist and genomic data) are often
highly-skewed distributions.

How is it solved?

Similarly to Weibull regression for lifetime data analysis and survival
analysis for continuous response variables, a regression model for a
discrete variable based on the discrete Weibull distribution report a
good fit in comparison with other distributions for count data.

Features of discrete Weibull distribution such as -Poissonmixtures,
-Poisson-Tweedie, -zero-inflated semi-parametric regression and
-COMPoisson -the ability to capture both over and under-dispersion and a
closed-form analytical expression of the quantiles of the conditional
distribution make it an alternative to the more traditional Poisson and
Negative Binomial distributions.

Non-informative priors and the Laplace priors with a hyper penalty
parameter calculated posterior distribution which is proper with finite
moments under a uniform non-informative prior.

Results

The advantage of Bayesian approaches over classical maximum likelihood
inference are: (1) the possibility of taking prior information into
account, such as sparsity or information from historical data, (2) the
procedure returns automatically the distribution of all parameters, from
which credible intervals can easily be obtained.

Application

The study compared the proposed model with the Bayesian Poisson
(BPoisson), Bayesian Negative Binomial (BNB) models and Bayesian DW
model on three datasets (inhaler use, health survey, health registry),
where BDW(regQ,β) models showed superior performance to the other
models. The Bayesian discrete Weibull model shows applicability in
analysing count data from the medical domain.\@Haselimashhadi2018

8.Bayesian Multiple Imputation and Logistic regression

Missing data, common occurrence in clinical research occurs when the
values of the variables of interest are not measured or recorded for all
subjects in the sample due to - (i) patient refusal to respond to
specific questions (ii) loss of patient to follow-up; (iii)investigator
or mechanical error (iv) physicians not ordering certain investigations
for some patients

The study mention the importance of understanding the type of missing
data (MAR, MNAR, MCAR), based on the type missing data, Multiple
imputation (MI) is a popular approach for addressing the presence of
missing data. Mi provides multiple plausible values of a given variable
imputed or filled in for each subject who has missing data for that
variable. This results in the creation of multiple completed data sets.

With this approach identical statistical analyses are conducted in each
of these complete data sets. The pooled results from across complete
data sets, are then analyzed.

The study introduces MI, its implementation, developing the imputation
model, emphasizing number of imputed data sets to create, and addresses
derived variables.

They provided application of MI through an analysis on patients
hospitalized with heart failure to estimate the probability of 1-year
mortality in the presence of missing data using (R, SAS, and
Stata)@Austin2021

## Method and Data Preparation

Statistical Tool used is R, R packages, and libraries to import data,
management and conduct analysis. Using haven package .XPT files were
converted to R-object as dataframe (df).

```{r}
#| label: Libraries
#| include: false


# loading packages 

library(tidyverse)
library(knitr)
library(ggthemes)
library(ggrepel)
library(dslabs)
library(Hmisc)
library(dplyr)
library(tidyr)
library(forcats)
library(ggplot2)
library ("nhanesA")                 

options(repos = c(CRAN = "https://cloud.r-project.org"))

install.packages("nhanesA")

```

Data Source - The retrospective study used NHANES 2-year cross-sectional
data (between 2013-2014 year) extracted from 3 datasets (demographics,
exam, questionnaire) @CenterforHealthStatistics1999.

Data Management

We created subsets from original 3 datasets (demographics, exam,
questionnaire) with selected variables of interest and then merged
ininto a single dataframes for analysis and data exploration. The merged
dataframe was cleaned, categorized, and recoded and analyzed for basic
statistics, anamolies and patterns before running Bayesian regression.
Final dataset included weighted means of all selected variables of
interest -

1.  Response Variable (Binary, Diabetes) was defined as - "Is there one
    Dr you see for diabetes"

2.  Predictor Variables (Body Mass Index, factor, 4 levels)

    The original data has BMDBMIC (measured BMI) as categorical and had
    no missing values. It (BMI_cat) has the following 4 levels:\
    o Underweight (\<5th percentile)\
    o Normal (5th–\<85th)\
    o Overweight (85th–\<95th) o Obese (≥95th percentile)\
    o Missing We kept it as it is as categorization provides clinically
    interpretable groups

3.  Covariates:

-   Gender (factor, 2 levels): Male: Female

-   Ethnicity (factor, 5 levels): Mexican American, Non-Hispanic, White
    Non-Hispanic, Black Other Hispanic, Other Race - Including
    Multi-Racial

-   Age (num, continuous)

```{r message=FALSE, warning=FALSE}
#| label: nhanes tables
#| include: false

 # making subsets for each dataset  
                       
                       nhanesTables('EXAM', 2013)
                       nhanesTables('QUESTIONNAIRE', 2013)
                       nhanesTables('DEMOGRAPHICS', 2013)
                       
                       
nhanesCodebook("BMX_H")
nhanesCodebook("SMQ_H")
           
  #  .xpt files read ( 2013–2014) 
                      bmx_h <- nhanes("BMX_H")         #Exam
                      smq_h <- nhanes("SMQ_H")         #Quest
                      demo_h <- nhanes("DEMO_H")       #Demo
                      diq_h <- nhanes("DIQ_H")         #diabetes


# variables of interest
exam_sub <-   bmx_h  %>% select(SEQN, BMDBMIC)
demo_sub <- demo_h %>% select(SEQN, RIDAGEYR, RIAGENDR, RIDRETH1, SDMVPSU, SDMVSTRA, WTMEC2YR)
diq_sub <-  diq_h %>% select (SEQN, DIQ240)

# Names of all variables selected for analysis
names(exam_sub)
names(demo_sub)
names(diq_sub)

# merged dataframe
merged_data <- exam_sub %>%
  left_join(demo_sub, by = "SEQN") %>%
  left_join(diq_sub, by = "SEQN")
head(merged_data)


nhanesCodebook("DEMO_H",'RIDRETH1')
nhanesCodebook("DEMO_H",'RIAGENDR')
nhanesCodebook("DEMO_H",'RIDAGEYR')
nhanesCodebook("DIQ_H","DIQ240")
nhanesCodebook("BMX_H",'BMDBMIC')


```

```{r}

install.packages("gt")   
library(gt)

variables <- c("SEQN", "BMDBMIC", "RIDAGEYR", "RIAGENDR", "RIDRETH1", 
               "SDMVPSU", "SDMVSTRA", "WTMEC2YR", "DIQ240")



df <- data.frame(Variable = variables, Description = descriptions <- c("Respondent sequence number", 
        "BMI calculated as weight in kilograms divided by height in meters squared, and then rounded to one decimal place.", 
                  "Age Age in years of the participant at the time of screening. Individuals 80 and over are topcoded at 80 years of age.", 
                  "Gender", 
                  "Race/ethnicity Recode of reported race and Hispanic origin information", 
                  "Sample PSU", 
                  "Sample strata", 
                  "MEC exam weight", 
                  "Diabetes status Is there one doctor or other health professional {you usually see/SP usually sees} for {your/his/her} diabetes? Do not include specialists to whom {you have/SP has} been referred such as diabetes educators, dieticians or foot and eye doctors."))
df %>%
  gt %>%
  tab_header(
    title = "Table Variable Description"
  ) %>%
  tab_footnote(
    footnote = "Each variable in the dataset, accompanied by a qualitative description from the study team."
  )
          

```

Weighted means and sd extracted from the datasets using library(survey)
are reported and the final merged dataframe included 9813 observations.

```{r}
#| label: weighted means
#| include: false

# weighted meand of each variable                       
library(survey)
nhanes_design <- svydesign(
  id = ~SDMVPSU,
  strata = ~SDMVSTRA,
  weights = ~WTMEC2YR,
  data = merged_data,
  nest = TRUE
)

# weighted mean of all variables
svymean(~RIDAGEYR, design = nhanes_design, na.rm = TRUE)
svymean(~SEQN, design = nhanes_design, na.rm = TRUE)
svymean(~BMDBMIC, design = nhanes_design, na.rm = TRUE)
svymean(~RIAGENDR, design = nhanes_design, na.rm = TRUE)
svymean(~RIDRETH1, design = nhanes_design, na.rm = TRUE)
svymean(~DIQ240, design = nhanes_design, na.rm = TRUE)



```

The unique categories, level, special codes and NAs of each variables
were explored.

```{r message=FALSE, warning=FALSE}
#| label: merged dataframe
#| echo: true

library(DataExplorer)

str(merged_data)
plot_str(merged_data)
introduce(merged_data)

plot_intro(merged_data, title="Figure 1. Structure of variables and missing observations.")

plot_missing(merged_data, title="Figure 2. Breakdown of missing observations.")

plot_bar(merged_data, title = "Figure 3. Frequency plots of categorical variables.")

plot_histogram(merged_data$RIDAGEYR, title = "Figure 4. Histogram plots of numerical variables.")

plot_qq(merged_data$RIDAGEYR, title = "Figure 5. QQ plots to assess normality of numerical variables age.")


```

## Explain your data preprocessing and cleaning steps.

Data Cleaning of the merged dataset revealed missing values (NA),
special codes (7,9,77,99). Considering special codes are not random and
cannot be dropped, based on the variable (codebook) they were
transformed into NAs. All NAs were included in the analysis, since R
automatically excludes rows with NA via listwise deletion or complete
case analysis during regression.

To explore missigness in the data, we conducted linear regression lm (),
and listwise deletion of rows resulted in a reduced sample size (n=14),
which could introduce bias as the informative missingness if ignored
(MAR or MNAR). The regression resulted in quasi-separation (warning).

## Raw Data Exploration and Visualization

-   We explored raw data and reported statistical summary,
    visualizations (histogram for continuous variables), and bar plot
    for categorical variables (BMI, gender, diabetes status, race)
    Figure 4 revealed most cases are non-hispanic whites. Of those who
    reported diabetes status, more counts reported having diabetes.
    Genders are relatively evenly distributed. Majority population were
    in the normal BMI range.
-   Histograms (Figure 5) shows age (continuous) frequency distributions
    with slight right skewness.
-   Missigness is presented as a breakdon in the graph.

```{r message=FALSE, warning=FALSE}
#| label: missing values_final data
#| include: false


sum(is.na(merged_data$DIQ240))   # number of missing in DIQ240
sum(is.na(merged_data$RIDAGEYR)) # number of missing in RIDAGEYR
sum(is.na(merged_data$BMDBMIC)) # number of missing in RIDAGEYR
sum(is.na(merged_data$RIAGENDR)) # number of missing in RIDAGEYR
sum(is.na(merged_data$SEQN)) # number of missing in RIDAGEYR

missing_codes <- c(7, 8, 9, 77, 88, 99, 777,999)
sum(merged_data$DIQ240 %in% missing_codes)
sum(merged_data$RIDAGEYR %in% missing_codes)
sum(merged_data$BMDBMIC %in% missing_codes)
sum(merged_data$RIAGENDR  %in% missing_codes)
sum(merged_data$SEQN  %in% missing_codes)


```

```{r}
#| include: false


clean_data <- merged_data %>% 
    filter(
    !is.na(DIQ240),
    !is.na(RIDAGEYR),
    !is.na(BMDBMIC),
    !is.na(RIAGENDR),
    !is.na(RIDRETH1),
    !is.na(SEQN)
     )

## filtering ##  row filter listwise removal

missing_codes <- c(7, 8, 9, 77, 88, 99, 777,999)     # checking special codes

sum(clean_data$diab %in% missing_codes)
sum(clean_data$age  %in% missing_codes)
sum(clean_data$BMI_cat %in% missing_codes)
sum(clean_data$gender  %in% missing_codes)
sum(clean_data$race  %in% missing_codes)

```

```{r}

plot_intro(clean_data, title="Figure 6. Breakdown of missing observations.")


```

The graph shows the data after removing all the NAs.

```{r}
#| label: Clean dataset
#| 
# Count NAs in all columns

colSums(is.na(merged_data))
colSums(is.na(clean_data))


```

We then recoded variables in the merged data, special codes were
included in NA category and the descriptive summary statistics presented
in tabulated form for continuous and categorical variables include
counts, frequencies, proportions mean and sd

```{r}
#| label: new columns
#| include: false
 
# cleaning of special characters(7,9,77,99) from merged_data (Raw data)

special_codes <- c(7, 9, 77, 99)  

cols_to_clean <- c("SEQN", "DIQ240","BMDBMIC","RIDAGEYR","RIAGENDR", "RIDRETH1", "SDMVPSU","SDMVSTRA", "WTMEC2YR")

# Loop over columns

for (v in cols_to_clean) {if (is.character(merged_data[[v]]) || is.factor(merged_data[[v]])) {
      merged_data[[v]][merged_data[[v]] %in% c("Don't know","Refused")] <- NA
    }
  }

summary(merged_data)  ## no removal of NAs in merged_data 
                      # NAs are handled in the model 


```

```{r}
#| label: NAs
#| include: false

# NAs present in RIDAGEYR,DIQ240, BMDBMIC# 

merged_data <- rename(merged_data,
       id = SEQN, # Respondent sequence number
       gender = RIAGENDR,            # Gender
       age = RIDAGEYR,               # Age in years at screening
       diab = DIQ240,                # Do you see a dr for diabetes
       BMI_cat = BMDBMIC,            # BMI
       race = RIDRETH1               #Race
       )

```

```{r}
#| label: summry_raw data
#| include: false


library(dplyr)

# 1. Response variable distribution
diab_summary <- merged_data %>%
  count(diab) %>%
  mutate(Proportion = round(n / sum(n), 3)) %>%
  rename(Category = diab, Count = n) %>%
  mutate(Variable = "Diabetes Status") %>%
  select(Variable, Category, Count, Proportion)

# 2. Continuous predictors (example: age)
cont_summary <- merged_data %>%
  summarise(
    Mean = mean(age, na.rm = TRUE),
    SD   = sd(age, na.rm = TRUE),
    Min  = min(age, na.rm = TRUE),
    Max  = max(age, na.rm = TRUE)
  ) %>%
  mutate(Variable = "Age") %>%
  select(Variable, everything())

# 3. Function for categorical variables
cat_summary <- function(df, var, name) {
  df %>%
    count({{var}}) %>%
    mutate(Proportion = round(n / sum(n), 3),
           Variable = name) %>%
    rename(Category = {{var}}, Count = n) %>%
    select(Variable, Category, Count, Proportion)
}

gender_summary <- cat_summary(merged_data, gender, "Gender")
BMI_summary    <- cat_summary(merged_data, BMI_cat, "BMI Category")
race_summary   <- cat_summary(merged_data, race, "Race")

# 4. Combine all into one table
final_table <- bind_rows(
  diab_summary,
  cont_summary,
  gender_summary,
  BMI_summary,
  race_summary
)


```

```{r}
#| label: crosstabulation and raw data summary

# corss-tabulation #

table(merged_data$diab, merged_data$BMI_cat,useNA="ifany")
table(merged_data$race, merged_data$diab, useNA="ifany") # before cleaning and imputation
table(merged_data$gender, merged_data$diab, useNA="ifany")

# Print with title
kable(final_table, caption = "Table 1. Descriptive Statistics of Study Variables")
```

```{r}
#| label: Raw Data Vizualization
#| 
# load a libraries
library(knitr) # fancy table
library(tidyverse) # load library tidyverse
library(classpackage)
library(ggplot2)
library(dplyr)



# Age distribution
ggplot(merged_data, aes(x = age)) +
  geom_histogram(binwidth = 5, fill = "skyblue", color = "black") +
  theme_minimal() +
  labs(title = "Age Distribution", x = "Age (years)", y = "Count")

# BMI category
ggplot(merged_data, aes(x = BMI_cat)) +
  geom_bar(fill = "salmon") +
  theme_minimal() +
  labs(title = "BMI Category Distribution", x = "BMI Category", y = "Count")

# Race category
ggplot(merged_data, aes(x = race)) +
  geom_bar(fill = "salmon") +
  theme_minimal() +
  labs(title = "Race Category Distribution", x = "Race Category", y = "Count")

# Gender
ggplot(merged_data, aes(x = gender)) +
  geom_bar(fill = "lightgreen") +
  theme_minimal() +
  labs(title = "Gender Distribution", x = "Gender", y = "Count")

# Outcome
ggplot(merged_data, aes(x = factor(diab))) +
  geom_bar(fill = "purple") +
  theme_minimal() +
  labs(title = "Diabetes Status (DIQ240)", x = "Diabetes (2 = No, 1 = Yes)", y = "Count")


# Cross - tabulation #

# DIQ240 by Age #
ggplot(merged_data, aes(x = age , y = factor(diab))) +
  geom_boxplot(fill = "lightblue") +
  theme_minimal() +
  labs(title = "Age by Diabetes Status", x = "Age", y = "Diabetes (2 = No, 1 = Yes)" )


#  DIQ240 by BMI  #

## ALL NAs ## 
ggplot(merged_data, aes(x = BMI_cat  , fill = factor(diab))) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Diabetes by BMI Category", x = "BMI Category", y = "Proportion", fill = "diab")   


# DIQ240 by Race  #
ggplot(merged_data, aes(x = race, fill = factor(diab)  )) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Diabetes by Race ", x = "Race ", y = "Proportion", fill = "diab")

# DIQ240 by Gender

ggplot(merged_data, aes(x = gender, fill = factor(diab)  )) +
  geom_bar(position = "fill") +
  theme_minimal() +
  labs(title = "Diabetes by gender ", x = "gender ", y = "Proportion", fill = "diab")


ggplot(merged_data) + 
    geom_point(aes(x = merged_data$age,  y = merged_data$diab), 
               na.rm = TRUE,
               show.legend = TRUE,
               position = "jitter") 


ggplot(merged_data) +
    geom_bar(aes(x = gender, fill = race))


ggplot(merged_data) + 
    geom_freqpoly(aes(x = age), binwidth = 5, na.rm = TRUE)  # age distribution#
```

**Multiple logistic regression**

We conducted Multiple linear regression on raw dataset. After complete
case analysis, listwise deletion of NAs, resulted in small sample size
(presented are first 6 deleted rows)., with perfect (or quasi-perfect)
separation. @van2012flexible.

We exlored the cause of missingness, revealing that the cause of
missingness (MAR and MNAR) are found common in healthcare and public
health datasets (plot below shows missingness)

**Firth (penalized) regression**

Firth regression, a frquestist approach use Jeffreys prior for bias
correction. It does not provide posterior and no sampling using MCMC (as
in contrast to bayesian logisitic regression). Firth (penalized)
regression was considered to handle quasi-separation, @DAngelo2025.

```{r}
#| label: MLR (with missingness)


m_raw<- glm(diab ~ BMI_cat + age + gender + race,
                       family = binomial, data = merged_data)  

 # quasi-complete separation #

m_raw
plot(m_raw)
head(na.action(m_raw))


library(logistf) # penalized regression #

# Firth logistic regression
m_firth <- logistf(diab ~ BMI_cat + age + gender + race,
                   data = merged_data)

# Summary of results
summary(m_firth)

# Odds ratios and 95% CIs
exp(coef(m_firth))
exp(confint(m_firth))


```

```{r}
#| label: ex_models
#| eval: false
#| include: false
m01<- glm(diab ~ BMI_cat, 
                       family = binomial, data = merged_data, na.action=na.omit)
m02<- glm(diab ~ BMI_cat + age, 
                       family = binomial, data = merged_data, na.action=na.omit)
m03<- glm(diab ~ BMI_cat + race, 
                       family = binomial, data = merged_data, na.action=na.omit)
```

```{python}
import pandas as pd






```

## Analysis and Results

## Unexpected reports, patterns or anomalies in raw **data**

**Linear regression and challenges of raw data**

-   We found quasi-complete separation in the raw data (9799
    observations dropped)
-   Reduced sample size with less number of complete cases (n=14).
-   The model is overfitted to this subset and cannot be generalized.
-   Huge coefficients (e.g., 94, –50, 73) and the tiny residual deviance
    suggest perfect separation and sparse data in some categories with
    very few observations, resulted in imbalance in the outcome (very
    few cases of 0 or 1).
-   Logistic regression cannot estimate stable coefficients when
    predictors perfectly classify the outcome.

**Firth regression (raw data) handled** quasi-separation with
coefficients as finite, but the sample size was reduced (n= 14) where
estimates are highly uncertain, wide confidence intervals → cannot make
strong claims about predictor effects.

**Multivariate Imputation by Chained Equations (MICE)** @JSSv045i03

To avoid sample loss and potential bias in subsequent analyses, we
considered multiple imputation (MI) as an alternative approach for the
given raw dataset. Flatness of the density, heavy tails, non-zero
peakedness, skewness and multimodality do not hamper the good
performance of multiple imputation for the mean structure in samples n
\> 400, even for high percentages (75%) of missing data in one variable
\@@van2012flexible. Multiple Imputation (MI) is a Bayesian Approach (use
popular mice package in R) and adds sampling variability to the
imputations. Iterative Imputation (MICE) imputes missing values of one
variable at a time, using regression models based on the other variables
in the dataset. This is a chain process, with each imputed variable
becoming a predictor for the subsequent imputation and the entire
process is repeated multiple times to create several complete datasets,
each reflecting different possibilities for the missing data. Each
variable is imputed using its own appropriate univariate regression
model.

We performed Multivariate Imputation by Chained Equations (MICE) to
handle missingness in the predictor (BMI) and in the response variable
(diabetes status).

**Results from MICE:**

-   MI on raw data resulted in filling imputed values with resulting
    9813 observations with no NAs. A comparative bar plot on missingness
    in the raw data and the imputed data is presented below.

-   A heatmap of the imputed dataset generated a correlation between
    **BMI categories** and **Diabetes status.** BMI dummy variables are
    strongly **negatively correlated.**

-   **There was** no strong linear association between BMI category and
    diabetes in the dataset. Chi-square calculation of categorical
    varaibels revealed p-value = 0.5461, which is \> 0.05 with no
    evidence of association.

```{r}
#| label: MICE
#| echo: true

## Multiple Imputation performed 

library(mice)
library("VIM")

# Subset variables for imputation in analytic_data df

vars <- c("id","BMI_cat",  "age", "gender", "race", "SDMVPSU",  "SDMVSTRA", "WTMEC2YR", "diab"  )
analytic_data <- merged_data[, vars]

# Run mice to create 5 imputed datasets

imputed_data <- mice(
  analytic_data,
  m = 5,              # number of imputed datasets
  method = 'pmm',     # predictive mean matching
  seed = 123
)

str(imputed_data)
print(imputed_data)
# First imputed dataset

Imputed_data1 <- complete(imputed_data, 1)

marginplot(
  Imputed_data1[, c("BMI_cat", "diab")],
  col = mdc(1:2, trans = FALSE),
  cex = 1.2,
  cex.lab = 1.2,
  cex.numbers = 1.3,
  pch = 19
)

# Checking missingness in imputed data

summary(Imputed_data1)
colSums(is.na(Imputed_data1))  # check total missing values

plot_intro(Imputed_data1, title="Figure 8. Structure of variables and missing observations.")

plot_bar(merged_data, title = "Figure 3. Frequency plots of categorical variables.")
plot_bar(Imputed_data1, title = "Figure 9. Frequency plots of categorical variables.")

plot_correlation(na.omit(Imputed_data1$BMI_cat, Imputed_data1$diab), maxcat=5L, title = "Figure 10. Correlation matrix between BMI categories and Diabetes Status.")

plot_correlation(
  na.omit(data.frame(BMI_cat = Imputed_data1$BMI_cat,
                     diab    = Imputed_data1$diab)),
  maxcat = 5L,
  title = "Figure 10. Correlation matrix between BMI categories and Diabetes Status."
)

# Cross-tabulation
tab <- table(Imputed_data1$BMI_cat, Imputed_data1$diab)
print(tab)

# Chi-square test of independence
chisq.test(tab)
```

## Modeling 

Bayesian data augmentation and Logistic regression @Austin2021

For the given raw dataset, Bayesian logistic Regression (GLM) is
considered to calculate the probability of having Diabetes by
incorporated default prior and using Bernoulli likelihood with a logit
link is the standard, interpretable model for risk b.

1.  Priors - Bayesian augmentation stabilizes estimates via priors and
    provides finite, interpretable estimates and provides more reliable
    inferences.

    -   Shrinkage is possible when including predictors (BMI, age,
        gender, race )

    -   Encoding prior beliefs predicts estimates when data are sparse
        (Prior knowledge from literature suggests obesity is linked to
        diabetes)

2.  Bayesian modeling plays nicely with multiple imputation or joint,
    propagating uncertainty instead of silently dropping cases.

3.  Calculated Posterior and credible intervals for both coefficients
    and individual risk make clinical interpretation and shared
    decision-making models.

4.  Bayesian logistic GLM gives an interpretable, survey-aware,
    uncertainty-quantified Diabetes risk model that’s robust to sparse
    subgroups and easy to update.

5.  It is easy for an update, a new NHANES cycle can update the
    calculated posterior rather than refitting from scratch, developing
    a live risk model.

6.  Bayesian gives full probability statements instead of only Odds
    Ratio (OR) with 94% probabilistic results (credible intervals), and
    not just p-values.

*The Logistic regression model is:*

$$ \text{logit}(P(Y_i=1)) = \beta_0 + \beta_1 \cdot Age_i + \beta_2 \cdot BMI_i + \beta_3 \cdot HTN_i + \beta_4 \cdot HDL_i + \beta_5 \cdot (HTN_i \times HDL_i) $$

*Linear Regression equation:*

$$ y_i = \beta_0 + \beta_1 X_1 +\varepsilon_i $$

**Comparision of multiple imputation and Bayesian data augmentation**

+---------------------------+---------------------------------------+
| **Multiple imputation**   | **Bayesian data augmentation**        |
+===========================+=======================================+
| -   frequentist approach  | -   performs missing data imputation  |
|     and requires no       |     and regression model fitting      |
|     priors, and has       |     simultaneously                    |
|     moderate flexibility  |                                       |
|                           | -   Markov Chain Monte Carlo (MCMC)   |
|                           |     draws samples from the joint      |
|                           |     posterior of regression           |
|                           |     parameters, missing values and    |
|                           |     provide complete datasets by      |
|                           |     extracting posterior means,       |
|                           |     credible intervals, and           |
|                           |     probabilities                     |
+---------------------------+---------------------------------------+
| -   handles missing       | -   performed on the data with        |
|     values first by       |     missingness                       |
|     imputation, performs  |                                       |
|     regression analysis,  | -   shrink extreme estimates back     |
|     pools results         |     toward plausible values           |
+---------------------------+---------------------------------------+
| -   propagate uncertainty | -   handles uncertainty in missing    |
|     added after analysis  |     values fully propagated through   |
|     (pooling).            |     the model, naturally handles      |
|                           |     small or sparse datasets and      |
|                           |     separation problems.              |
+---------------------------+---------------------------------------+

**Diagnostics performed before regression analysis**

(1) Modeling assumption check
(2) Correlation matrix, Cook's distance, influential points
(3) Model plot

```{r}
#| label: MLR assumptions
#| echo: true

# MLR on imputed data
# assumption check (# for correlation # )
# correlation matrix

m_imp1 <- glm(diab ~ age + gender + race + BMI_cat,
          data = Imputed_data1, 
          family = binomial)
m_imp1

# Get the predicted logit
Imputed_data1$logit <- predict(m_imp1, type = "link")  # 'link' gives log-odds

# Plot BMI vs logit

plot_imp1 <- ggplot(Imputed_data1, aes(x = age, y = logit)) +
  geom_point(alpha = 0.3) +
  geom_smooth(method = "loess") +
  labs(x = "RIDAGEYR", y = "Log-odds of Diabetes")

 plot_imp1  
  
 
# Calculate fitted values and residuals from the final model
    fitted_imputed1 <- fitted(m_imp1)
    residual_imputed1 <- residuals(m_imp1)
  

#Implication for modeling:

# collinearity check #

library(car)
    vif(m_imp1)  # VIF > 5 or 10 indicates multicollinearity 

# Check for influential points and outliers # 
library(broom)
    influence_m_imp1 <- broom::augment(m_imp1)
    influence_m_imp1

    Imputed_data1 <- Imputed_data1 %>%
    mutate(outlier =  if_else(abs(rstandard(m_imp1))>2.5, "Suspected", "Not Suspected"))

    Imputed_data1 %>% count(outlier)

    cooks(m_imp1)

# Visualization 
ggplot(influence_m_imp1, aes(x = .hat, y = .cooksd)) +
  geom_point() +
  labs(x = "Leverage", y = "Cook's Distance")

Imputed_data1 %>% ggplot(aes(x = BMI_cat, y = diab, color = outlier)) +
  geom_point() + 
  scale_color_manual(values = c("#999999", "#000000")) +
  labs(x = "age", y = "diabetes", color = "Outlier") +
  theme_bw()

# transform response variable to codes

Imputed_data1$diab_num <- ifelse(Imputed_data1$diab == "Yes", 1, 0)

library(ResourceSelection)
hoslem.test(Imputed_data1$diab_num, fitted(m_imp1))
summary(m_imp1)


anova (m_imp1)
library(glmtoolbox)
(adjR2(m_imp1))

# plot- m_raw and m3 # residual vs fitted #

plot3 <- plot(m_imp1$fitted.values, resid(m_imp1),   ## residual vs fitted plot - imputed data ##
     xlab = "Fitted values",
     ylab = "Residuals",
     main = "Residuals vs Fitted",
     pch = 19, col = "blue")
abline(h = 0, col = "red", lwd = 2)


plot1 <- plot(m_raw$fitted.values, resid(m_raw),   ## residual vs fitted plot -  raw data ##
     xlab = "Fitted values",
     ylab = "Residuals",
     main = "Residuals vs Fitted",
     pch = 19, col = "blue")
abline(h = 0, col = "red", lwd = 2)

# See numeric codes (1 = Yes, 2 = No)
table(as.numeric(Imputed_data1$diab), useNA = "ifany")


```

Findings from regression of MI data set

1.  MLR on imputed data (Frequentist approach)

-   Relationship between age and log-odds of diabetes are roughly linear
    but not perfectly, but are acceptable for logistic regression
    assumptions.

-   Generalized Variance Inflation Factor (vif- adjusted report there is
    no collinearity between predictors (GVIF between \~1.0–1.04) and we
    can run model without removing or dropping or combining variables.

-   Cooks distance and influential points, we found - Most data points
    are safe, not influencing the model In the data with (\~9813 cases),
    cutoff ≈ 0.0004. A cluster at high leverage shows unusual predictor
    values, but not high influence. A few above Cook’s Distance cutoff:
    worth checking individually, but no major threat to model stability.
    no outliers detected (not suspected = 9813)

-   Results from Hosmer–Lemeshow (H–L) at alpha =0.05, with p \< 0.001,
    we find our logistic regression model does not fit the data well.

-   Graph shows Residual vs fitted (imputed data model)

2.  Results from Bayesian Data Augmentation and logistic regression

-   We incorporate prior knowledge that BMI increases diabetes odds by
    .,
-   We use priors for Bayesian logistic regression and compare the
    models with different priors in the model
    -   Prior (intercept) - We use intercept prior from this study data
    -   Prior (coefficients) - BMI, age, gender
        -   Weak prior N (0,2.5) -βBMI∼N(μ,σ2)
        -   A common approach is to use a normal distribution,
            βBMI∼N(μ,σ2), for the regression coefficient. 
        -   informative prior from previous studies βBMI∼N(μ,σ2) ,
            βage∼N(μ,σ2), βgender∼N(μ,σ2), βrace ∼N(μ,σ2)

For males, the informative prior @Ali2024, we use is

Normal(μ = 1.705, σ² = 0.448²).

```{r}
#| label: m_raw vs m_imp1
#| eval: false
#| include: false

# Model (classic, raw data)
m_raw <- glm(diab ~ age + BMI_cat + race + gender                                   , 
    family = binomial, data = merged_data)

# Model (classic, imputed data)
m_imp1 <- glm(diab ~ age + BMI_cat + race + gender                                   , 
    family = binomial, data = Imputed_data1)

print(m_imp1)


m_raw
m_imp1
m3

anova(m_raw)
anova(m_imp1)

adjR2(m_raw)
adjR2(m_imp1)

 # Q-Q Plot
      qqPlot(residual_imputed1, main = "Figure 13. Q-Q Plot of Residuals")
```

In m1 = race is a significant predictor

In m2 = age is strongly significant, race, and gender are slightly
significant predictors

```{r}
#| label: Bayesian augmentation MCMC
#| eval: false
#| include: false

library(brms)
library(GGally)

## correlation observed in the merged data before augmentation

merged_data_corr <- merged_data %>% select(diab, age)
pairs(merged_data_corr)

merged_data_corr <- merged_data %>% select(diab, BMI_cat)
pairs(merged_data_corr)


merged_data_corr1 <- merged_data %>% select(BMI_cat, gender, race)pairs(merged_data_corr)



## Bayesian data augmentation and regression both together #

merged_data$diab <- ifelse(merged_data$diab == "Yes", 1,
                           ifelse(merged_data$diab == "No", 0, NA))


## Augmentation and Prior ##

# For intercept prior (Nhanes dataset) 
# informative prior = intercept #

svymean(~DIQ240, design = nhanes_design, na.rm = TRUE)   # NHANES prevalence 
p <- 0.78759
logit_intercept <- log(p / (1 - p))     # mean #
logit_intercept


SE_prob <- 0.021               # SD #
SD_logit <- SE_prob / (p * (1 - p))   
SD_logit

# intercept prior = N (1.3, 0.13) #

# prior for coefficients (BMI_cat, age, race, gender) 

# Weakly informative priors

priors <- c(
  set_prior("normal(0, 2.5)", class = "b"),       # coefficients
  set_prior("normal(0, 5)", class = "Intercept")  # intercept
)

# bf() lets us handle missing predictors and outcomes
formula_bayes <- bf(
  diab | mi() ~ age + mi(BMI_cat) + gender + race
)

fit_bayes <- brm(
  formula = formula_bayes,
  data = merged_data,
  family = bernoulli(),   # logistic regression
  prior = priors,
  chains = 4,
  iter = 2000,
  seed = 123,
  control = list(adapt_delta = 0.95)  # helps with convergence
)


# Posterior summary
summary(fit_bayes)

# 95% credible intervals
posterior_interval(fit_bayes, prob = 0.95)

# Posterior predictive check
pp_check(fit_bayes)


# Extract imputed values for BMI_cat
imputed_BMI <- fitted(fit_bayes, summary = FALSE)[, "BMI_cat"]
head(imputed_BMI)

plot(fit_bayes)

imputed_modes <- apply(imputed_BMI[, is.na(merged_data$BMI_cat)], 2, function(x) {
  names(sort(table(x), decreasing = TRUE))[1]
})

# Fit Bayesian logistic regression with data augmentation


fit_bayes <- brm(
  formula =form_all,
  data = merged_data,
  prior = c(
    set_prior("normal(0, 2.5)", class = "b")  # weakly informative prior
  ),
  chains = 4, iter = 2000, cores = 4, seed = 123
)

summary(fit_bayes)
```

-   **The cluster of points representing the higher probability of
    diabetes appears to be denser among individuals in the middle to
    older age ranges (e.g., roughly from 40 to 80 years old)**, compared
    to the younger age ranges, although diabetes is present even at
    younger ages.

**Comparing Models**

-   Linear regression model on raw data

-   Bayesian logistic regression on imputed dataset (MI)

-   Bayesian data augmention and logistic regression

### Conclusion

-   Summarize your key findings.

-   Discuss the implications of your results.

## References
